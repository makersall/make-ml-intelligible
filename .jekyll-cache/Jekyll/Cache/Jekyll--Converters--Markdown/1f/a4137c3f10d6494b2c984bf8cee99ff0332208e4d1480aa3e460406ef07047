I"¼<p>So, what is â€œtrainingâ€?  (Lesson 2)
Training is how we</p>
<ul>
  <li>create a mathematical function</li>
  <li>that takes a bunch of numbers â€“ eg., the numbers that represent the pixels of a picture of â€œ8â€</li>
  <li>and spits out the probabilities for each possible class/answer â€“ eg., 92% that itâ€™s an â€˜8â€™</li>
</ul>

<p>So, how do we create a function that does that?</p>
<ul>
  <li>Pictures are too complicated to do the math on,</li>
  <li>so weâ€™ll start w something way, way simpler:  given temperature, predict how many ice creams sold (Lesson 2â€™s ex)</li>
</ul>

<p><strong>NOTE:I think from here on, itâ€™s mostly Jeremy; need to check for whatever I decided to keep</strong></p>

<p>The simplest function you can do:  for a line</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>y = mx + b
m: "gradient" of the line
</code></pre></div></div>

<p>For simplest v of what Gradient is: itâ€™s another word for â€œslopeâ€
(In vector calculus, the gradient is a multi-variable generalization of the derivative)</p>

<p>b: â€œinterceptâ€ of the line</p>

<p>Only what weâ€™re actually going to use is
y = a1x1 + a2x2
where x2 is 1</p>

<p>Why? 
(not sure yet.  J says: in machine learning, we donâ€™t hae one equation, we got lots)</p>

<p>or:
y[i] = a1x[i,1] + a2x[i,2]</p>

<p>a1 and a2 are the coefficients or parameters</p>

<p>This lets us deal w it using linear algebra</p>

<p>NOTE: multiplying matrices, you multiply pieces and then add them up, aka â€œdot product.â€ Do it for each item in the matrix and youâ€™ve got a matrix product</p>

<p>[I think]
For our â€œxâ€ data, create a â€œtensorâ€ of 2 columns:
col 1: a random number between -1 and 1
col 2: 1 â€“ which is the intercept??  not sure.  Or itâ€™s there to make the matrix multiplication possible?</p>

<p>For â€œaâ€, create a â€˜rank 1â€™ tensor of (3., 2)
rank = how many axes/dimensions
in math world, rank 1 is a vector, rank 2 is a matrix</p>

<p>(3,2) was the correct answer.
i,e, that m, aka a1, was 3 and b, aka a2 was 2.</p>

<p>What if we didnâ€™t know the correct answers were 3,2?<br />
Theyâ€™re what stats people call coefficient and PyTorch calls parameters</p>

<p>Loss function: function we can run on the predicted vs actual numbers to tell us how accurate our prediction, aka the line generated by our parameters, was.</p>

<p>The basic strategy:
start with a wild ass guess for the parameters</p>

<p>Then we calculate the difference between this guess and the right answer (from the training data),
then make the guess a bit better
For a line, which has only 2 parameters, we ask:
what if we made the intercept a bit higher or lower?  What if we made the gradient/slope a bit more positive or negative?</p>

<p>2 parameters X 2 options = 4 possible outcomes.
So we calculate the loss for each of the 4, then whichever did the best, thatâ€™s what we do
Only we donâ€™t have to literally move it up/down,
we can calculate the derivative, which tells us what moving it around wouldâ€™ve done
J: The derivativ â€œtells you how changing one thing change the functionâ€
The derivative is kinda sorta, close enough, to the gradient â€“ how changing it ujp or down would change how close we were
(in PyTorch, you calculate the gradient using â€œbackwardâ€ method)
Learning rate is the amount we multiply the gradient by â€“ essentially how big of a jump we do</p>

<p>He suggests running the animation of SGD using a big learning rate, a small learning rate, etc to get a feel for it</p>

<p>stochastic gradient descent: do that, but donâ€™t do it on your whole dataset, do it on mini-batches of your data.  If youâ€™ve got 1 million images and you do all of them, that means youâ€™re calculating the loss function on 1 million images every single time.</p>

<p>The danger w epochs:  if you try to fit your data too many times, if you look at each image too many times, itâ€™ll start overfitting â€“ working too exactly w thos images</p>

<blockquote>
  <p>So when we created that teddy bear detector, what we actually did was we created a mathematical function that took the numbers from the images of the teddy bears and a mathematical function converted those numbers into, in our case, three numbers: a number for the probability that itâ€™s a teddy, a probability that itâ€™s a grizzly, and the probability that itâ€™s a black bear.</p>
</blockquote>

<blockquote>
  <p>In this case, thereâ€™s some hypothetical function thatâ€™s taking the pixel representing a handwritten digit and returning ten numbers: the probability for each possible outcome (i.e. the numbers from zero to nine).  So what youâ€™ll often see in our code and other deep learning code is that youâ€™ll find this bunch of probabilities and then youâ€™ll find a function called max or argmax attached to it. What that function is doing is, itâ€™s saying find the highest number (i.e. probability) and tell me what the index is. So np.argmax or torch.argmax of the above array would return the index 8.</p>
</blockquote>

:ET