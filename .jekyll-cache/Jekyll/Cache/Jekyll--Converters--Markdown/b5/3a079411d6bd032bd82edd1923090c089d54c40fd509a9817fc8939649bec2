I"¼<h2 id="overview">Overview</h2>

<p>Babies:</p>

<ul>
  <li>Underfitting: â€œdogâ€ = dogs and any other pets</li>
  <li>Overfitting: â€œdogâ€ = only our familyâ€™s dog</li>
</ul>

<p>Or:</p>

<ul>
  <li>Overfitting: the model only learns to recognize blenders in these particular photos and not pictures in general of blenders</li>
</ul>

<h2 id="overfitting">Overfitting</h2>

<p>The only way to get an idea of whether your overfitting is to use a validation set â€“ a set of data that the model doesnâ€™t train on</p>

<h2 id="overfitting-validation-set">Overfitting, Validation Set</h2>

<blockquote>
  <p>Wait a minute, how do you know it can actually recognize pictures of people playing cricket versus baseball in general? Maybe it just learnt to recognize those 30. Maybe itâ€™s just cheating. Thatâ€™s called â€œoverfittingâ€. Weâ€™ll be talking a lot about that during this course. But overfitting is where you donâ€™t learn to recognize pictures of say cricket versus baseball, but just these particular cricketers in these particular photos and these particular baseball players in these particular photos. We have to make sure that we donâ€™t overfit. The way to do that is using something called a validation set. A validation set is a set of images that your model does not get to look at. So these metrics (e.g. error_rate) get printed out automatically using the validation set - a set of images that our model never got to see. When we created our data bunch, it automatically created a validation set for us. Weâ€™ll learn lots of ways of creating and using validation sets, but because weâ€™re trying to bake in all of the best practices, we actually make it nearly impossible for you not to use a validation set. Because if youâ€™re not using a validation set, you donâ€™t know if youâ€™re overfitting. So we always print out the metrics on a validation, weâ€™ve always hold it out, we always make sure that the model doesnâ€™t touch it.</p>
</blockquote>

:ET